{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Image Classification for Small Data\n",
    "This notebook uses the Keras framework to train an image classifier using a small dataset.\n",
    "\n",
    "Source: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Keras==2.2.0\n",
      "  Using cached https://files.pythonhosted.org/packages/68/12/4cabc5c01451eb3b413d19ea151f36e33026fc0efb932bf51bcaf54acbf5/Keras-2.2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: h5py in /usr/local/envs/py3env/lib/python3.5/site-packages (from Keras==2.2.0) (2.7.1)\n",
      "Collecting keras-applications==1.0.2 (from Keras==2.2.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/e2/60/c557075e586e968d7a9c314aa38c236b37cb3ee6b37e8d57152b1a5e0b47/Keras_Applications-1.0.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/envs/py3env/lib/python3.5/site-packages (from Keras==2.2.0) (1.0.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from Keras==2.2.0) (1.10.0)\n",
      "Collecting keras-preprocessing==1.0.1 (from Keras==2.2.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/f8/33/275506afe1d96b221f66f95adba94d1b73f6b6087cfb6132a5655b6fe338/Keras_Preprocessing-1.0.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyyaml in /usr/local/envs/py3env/lib/python3.5/site-packages (from Keras==2.2.0) (3.13)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/envs/py3env/lib/python3.5/site-packages (from Keras==2.2.0) (1.14.0)\n",
      "Installing collected packages: keras-applications, keras-preprocessing, Keras\n",
      "Successfully installed Keras-2.2.0 keras-applications-1.0.2 keras-preprocessing-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install Keras==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras import applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data comes in the following structure:\n",
    "    \n",
    "    data/ <br />\n",
    "    --train/<br />\n",
    "    --- dogs/<br />\n",
    "    ----- dog.001.jpg<br />\n",
    "    ----- dog.002.jpg<br />\n",
    "    ----- ...<br />\n",
    "    --- cats/<br />\n",
    "    ----- cat.001.jpg<br />\n",
    "    ----- cat.002.jpg<br />\n",
    "    ----- ...<br />\n",
    "    -- validation/<br />\n",
    "    --- dogs/<br />\n",
    "    ----- dog.001.jpg<br />\n",
    "    ----- dog.002.jpg<br />\n",
    "    ----- ...<br />\n",
    "    --- cats/<br />\n",
    "    ----- cat.001.jpg<br />\n",
    "    ----- cat.002.jpg<br />\n",
    "    ----- ...<br />\n",
    "    \n",
    " We have to load it from a GCP bucket: gsutil cp -m gs://small-image-classifier/ ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'validation', 'predictions', 'preview']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing and data augmentation\n",
    "\n",
    "Apply random transformations, so that the model would never see the exact same picture twice. This helps prevent overfitting and helps the model generalize better.\n",
    "\n",
    "See preview how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "img = load_img('../data/train/cats/cat.0.jpg')  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1,\n",
    "                          save_to_dir='../data/preview', save_prefix='cat', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat_0_7841.jpeg',\n",
       " 'cat_0_4059.jpeg',\n",
       " 'cat_0_8366.jpeg',\n",
       " 'cat_0_9858.jpeg',\n",
       " 'cat_0_9545.jpeg',\n",
       " 'cat_0_3888.jpeg',\n",
       " 'cat_0_8745.jpeg',\n",
       " 'cat_0_5072.jpeg',\n",
       " 'cat_0_3557.jpeg',\n",
       " 'cat_0_9864.jpeg',\n",
       " 'cat_0_2143.jpeg',\n",
       " 'cat_0_6824.jpeg',\n",
       " 'cat_0_2663.jpeg',\n",
       " 'cat_0_4980.jpeg',\n",
       " 'cat_0_6900.jpeg',\n",
       " 'cat_0_2499.jpeg',\n",
       " 'cat_0_3717.jpeg',\n",
       " 'cat_0_9898.jpeg',\n",
       " 'cat_0_8265.jpeg',\n",
       " 'cat_0_3451.jpeg',\n",
       " 'cat_0_8216.jpeg']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../data/preview/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 150, 150\n",
    "train_data_dir = '../data/train'\n",
    "validation_data_dir = '../data/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure Input Shapes are formatted correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "print(input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is our first model, a simple stack of 3 convolution layers with a ReLU activation and followed by max-pooling layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# the model so far outputs 3D feature maps (height, width, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of it we stick two fully-connected layers. We end the model with a single unit and a sigmoid activation, which is perfect for a binary classification. To go with it we will also use the binary_crossentropy loss to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten()) # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare our data. We will use .flow_from_directory() to generate batches of image data (and their labels) directly from our jpgs in their respective folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,  # this is the target directory\n",
    "        target_size=(img_width, img_height),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "125/125 [==============================] - 68s 547ms/step - loss: 0.7094 - acc: 0.5235 - val_loss: 0.7026 - val_acc: 0.5262\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feed5d0a450>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_save_weights('first_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-bcee9ba30314>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'first_try.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.load_weights('first_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 72, 72, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                1183808   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,212,513\n",
      "Trainable params: 1,212,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and save the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten()) # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "model.load_weights('first_try.h5')\n",
    "model.save('model_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building on top of a pre-trained model\n",
    "\n",
    "To improve model accuracy, instead of training a model from scratch, we take a pre-trained model and customize it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model_weights_path = 'model_2_weights.h5'\n",
    "model_2 = 'model_2.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to save bottleneck features from pre-trained VGG16 network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bottleneck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    # build the VGG16 network\n",
    "    \n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "        \n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        generator, nb_train_samples // batch_size)\n",
    "    \n",
    "    np.save(open('bottleneck_features_train.npy', 'wb'),\n",
    "            bottleneck_features_train)\n",
    "    \n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "        \n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, nb_validation_samples // batch_size)\n",
    "    \n",
    "    np.save(open('bottleneck_features_validation.npy', 'wb'),\n",
    "            bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to train the top part of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_top_model():\n",
    "    train_data = np.load(open('bottleneck_features_train.npy', 'rb'))\n",
    "    \n",
    "    train_labels = np.array(\n",
    "        [0] * int(nb_train_samples / 2) + [1] * int(nb_train_samples / 2))\n",
    "    \n",
    "    validation_data = np.load(open('bottleneck_features_validation.npy', 'rb'))\n",
    "    \n",
    "    validation_labels = np.array(\n",
    "        [0] * int(nb_validation_samples / 2) + [1] * int(nb_validation_samples / 2))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    \n",
    "    model.save_weights(top_model_weights_path)\n",
    "    model.save(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "save_bottleneck_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 800 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 0.6985 - acc: 0.7700 - val_loss: 0.2762 - val_acc: 0.8962\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.3729 - acc: 0.8465 - val_loss: 0.2562 - val_acc: 0.8938\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.3121 - acc: 0.8850 - val_loss: 0.2474 - val_acc: 0.9000\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.2565 - acc: 0.9025 - val_loss: 0.2445 - val_acc: 0.8988\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.2152 - acc: 0.9155 - val_loss: 0.2826 - val_acc: 0.9012\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.2031 - acc: 0.9195 - val_loss: 0.2795 - val_acc: 0.9025\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1779 - acc: 0.9380 - val_loss: 0.2946 - val_acc: 0.9038\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.1545 - acc: 0.9390 - val_loss: 0.3494 - val_acc: 0.8925\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.1448 - acc: 0.9475 - val_loss: 0.3681 - val_acc: 0.9025\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.1244 - acc: 0.9490 - val_loss: 0.5627 - val_acc: 0.8588\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.1123 - acc: 0.9575 - val_loss: 0.4401 - val_acc: 0.8788\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0996 - acc: 0.9645 - val_loss: 0.3861 - val_acc: 0.8962\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0821 - acc: 0.9680 - val_loss: 0.4342 - val_acc: 0.8988\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0782 - acc: 0.9725 - val_loss: 0.5009 - val_acc: 0.9000\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0795 - acc: 0.9655 - val_loss: 0.5205 - val_acc: 0.8912\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0656 - acc: 0.9755 - val_loss: 0.5280 - val_acc: 0.9000\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0652 - acc: 0.9740 - val_loss: 0.4996 - val_acc: 0.9000\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0533 - acc: 0.9790 - val_loss: 0.5613 - val_acc: 0.8912\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0526 - acc: 0.9820 - val_loss: 0.5706 - val_acc: 0.8988\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0408 - acc: 0.9840 - val_loss: 0.6280 - val_acc: 0.9000\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0439 - acc: 0.9860 - val_loss: 0.6130 - val_acc: 0.9012\n",
      "Epoch 22/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0333 - acc: 0.9880 - val_loss: 0.6632 - val_acc: 0.9025\n",
      "Epoch 23/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0513 - acc: 0.9840 - val_loss: 0.6515 - val_acc: 0.8950\n",
      "Epoch 24/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0279 - acc: 0.9910 - val_loss: 0.7639 - val_acc: 0.9000\n",
      "Epoch 25/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0413 - acc: 0.9895 - val_loss: 0.8597 - val_acc: 0.8788\n",
      "Epoch 26/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0404 - acc: 0.9850 - val_loss: 0.7524 - val_acc: 0.8938\n",
      "Epoch 27/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0300 - acc: 0.9885 - val_loss: 0.6876 - val_acc: 0.8950\n",
      "Epoch 28/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0312 - acc: 0.9885 - val_loss: 0.8814 - val_acc: 0.8788\n",
      "Epoch 29/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0191 - acc: 0.9935 - val_loss: 0.8133 - val_acc: 0.8925\n",
      "Epoch 30/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0278 - acc: 0.9910 - val_loss: 0.8353 - val_acc: 0.8962\n",
      "Epoch 31/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0206 - acc: 0.9930 - val_loss: 0.8914 - val_acc: 0.8962\n",
      "Epoch 32/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0257 - acc: 0.9900 - val_loss: 0.8707 - val_acc: 0.8900\n",
      "Epoch 33/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0308 - acc: 0.9920 - val_loss: 0.7750 - val_acc: 0.9000\n",
      "Epoch 34/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0272 - acc: 0.9925 - val_loss: 0.8247 - val_acc: 0.9025\n",
      "Epoch 35/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0165 - acc: 0.9940 - val_loss: 0.7958 - val_acc: 0.8975\n",
      "Epoch 36/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0228 - acc: 0.9945 - val_loss: 0.8728 - val_acc: 0.9000\n",
      "Epoch 37/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0260 - acc: 0.9935 - val_loss: 0.8360 - val_acc: 0.9012\n",
      "Epoch 38/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0174 - acc: 0.9950 - val_loss: 0.8608 - val_acc: 0.9038\n",
      "Epoch 39/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0277 - acc: 0.9935 - val_loss: 0.9379 - val_acc: 0.8938\n",
      "Epoch 40/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0175 - acc: 0.9960 - val_loss: 0.8978 - val_acc: 0.8938\n",
      "Epoch 41/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0183 - acc: 0.9945 - val_loss: 0.8908 - val_acc: 0.8988\n",
      "Epoch 42/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0354 - acc: 0.9930 - val_loss: 0.8466 - val_acc: 0.9038\n",
      "Epoch 43/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0162 - acc: 0.9960 - val_loss: 0.9005 - val_acc: 0.8938\n",
      "Epoch 44/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0139 - acc: 0.9945 - val_loss: 0.9242 - val_acc: 0.8938\n",
      "Epoch 45/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0242 - acc: 0.9920 - val_loss: 0.8768 - val_acc: 0.8912\n",
      "Epoch 46/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0145 - acc: 0.9965 - val_loss: 0.9824 - val_acc: 0.8950\n",
      "Epoch 47/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0178 - acc: 0.9945 - val_loss: 1.0370 - val_acc: 0.8888\n",
      "Epoch 48/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0188 - acc: 0.9965 - val_loss: 0.9344 - val_acc: 0.8950\n",
      "Epoch 49/50\n",
      "2000/2000 [==============================] - 4s 2ms/step - loss: 0.0165 - acc: 0.9950 - val_loss: 0.9670 - val_acc: 0.8938\n",
      "Epoch 50/50\n",
      "2000/2000 [==============================] - 5s 2ms/step - loss: 0.0128 - acc: 0.9960 - val_loss: 1.0000 - val_acc: 0.8938\n"
     ]
    }
   ],
   "source": [
    "train_top_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
