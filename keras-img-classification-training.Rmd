---
title: "Keras Small Data Image Classifier - Training"
author: "Tobias Zwingmann"
date: "17 2 2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("reticulate")
```


```{python}
import sys
print(sys.version_info)
import os
```

```{python}
from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras import backend as K
```


```{python}
import PIL
img = load_img('../data/small-image-classifier/data/train/cats/cat.0.jpg')
keras.preprocessing.image.img_to_array(img)

```

## Data Preparation

Data comes in the following structure: 

data/
    train/
        dogs/
            dog.001.jpg
            dog.002.jpg
            ...
        cats/
            cat.001.jpg
            cat.002.jpg
            ...
    validation/
        dogs/
            dog.001.jpg
            dog.002.jpg
            ...
        cats/
            cat.001.jpg
            cat.002.jpg
            ...

and must be loaded to the local VM from GCS if not exists with this command 

```{python}
if  not os.path.exists('../data/small-image-classifier'):
    os.system('mkdir ../data/')
    os.system('gsutil cp -r gs://small-image-classifier/ ../data/')
```

```{python}
print(os.listdir('../data/'))
```

# Data pre-processing and data augmentation

Apply random transformations, so that the model would never see the exact same picture twice. This helps prevent overfitting and helps the model generalize better.

See preview how it works:

```{python}
datagen = ImageDataGenerator(
        rotation_range=40,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest')

img = load_img('../data/small-image-classifier/data/train/cats/cat.0.jpg')  # this is a PIL image
x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)
x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)

# the .flow() command below generates batches of randomly transformed images
# and saves the results to the `preview/` directory
i = 0
for batch in datagen.flow(x, batch_size=1,
                          save_to_dir='../data/small-image-classifier/data/preview', save_prefix='cat', save_format='jpeg'):
    i += 1
    if i > 20:
        break  # otherwise the generator would loop indefinitely
```

## Modeling

### Variables
```{python}
img_width, img_height = 150, 150
train_data_dir = '../data/small-image-classifier/data/train'
validation_data_dir = '../data/small-image-classifier/data/validation'
nb_train_samples = 2000
nb_validation_samples = 800
epochs = 50
batch_size = 16
```

```{python}
print(os.listdir(validation_data_dir))
```

Make sure Input Shapes are formatted correctly:

```{python}
if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height)
else:
    input_shape = (img_width, img_height, 3)
```

```{python}
print(input_shape)
```

Below is our first model, a simple stack of 3 convolution layers with a ReLU activation and followed by max-pooling layers.

```{python}
model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=input_shape))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

# the model so far outputs 3D feature maps (height, width, features)
```

On top of it we stick two fully-connected layers. We end the model with a single unit and a sigmoid activation, which is perfect for a binary classification. To go with it we will also use the binary_crossentropy loss to train our model.

```{python}
model.add(Flatten()) # this converts our 3D feature maps to 1D feature vectors
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])
```

Let's prepare our data. We will use .flow_from_directory() to generate batches of image data (and their labels) directly from our jpgs in their respective folders.

```{python}
# this is the augmentation configuration we will use for training
train_datagen = ImageDataGenerator(
        rescale=1./255,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True)

# this is the augmentation configuration we will use for testing:
# only rescaling
test_datagen = ImageDataGenerator(rescale=1./255)

# this is a generator that will read pictures found in
# subfolers of 'data/train', and indefinitely generate
# batches of augmented image data
train_generator = train_datagen.flow_from_directory(
        train_data_dir,  # this is the target directory
        target_size=(img_width, img_height),  # all images will be resized to 150x150
        batch_size=batch_size,
        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels

# this is a similar generator, for validation data
validation_generator = test_datagen.flow_from_directory(
        validation_data_dir,
        target_size=(img_width, img_height),
        batch_size=batch_size,
        class_mode='binary')
```

## Model Training
```{python}
model.fit_generator(
        train_generator,
        steps_per_epoch=nb_train_samples // batch_size,
        epochs=epochs,
        validation_data=validation_generator,
        validation_steps=nb_validation_samples // batch_size)
```

